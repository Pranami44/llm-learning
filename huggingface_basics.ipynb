{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fe9eb1-ba9c-44c9-a040-9f091acc2b1f",
   "metadata": {},
   "source": [
    "# Hugging Face Basics\n",
    "- How to load and use pre-trained models from HuggingFace.  \n",
    "- Difference between model, tokenizer, and pipeline.  \n",
    "- Running a text generation pipeline with GPT-2.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77177160-9018-4ae9-bf60-fe24cf99ab0c",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "- HuggingFace pipeline is a high-level API that lets you quickly use pretrained models for tasks like text-generation, sentiment-analysis, translation, etc.\n",
    "- It abstracts away the details of loading tokenizer + model + running inference.\n",
    "- It takes a prompt, predicts next tokens, and returns generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aced03a-97ae-4bbf-80c2-15f98982a612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, today I am learning about the origins of the 'Birds of a Feather' species. These are the four species of birds that live in the New World. They are the only two species of birds that can fly. They are the only two birds to have a wing, yet have a wingspan of nearly 30 inches. They are the only birds that can fly. They are the only two birds to have a wing, yet have a wingspan of as much as 20 inches. And they are the only birds that can fly. They are the only two species of birds that can fly. They are the only two species of birds that can fly. They are the only two species of birds that can fly. They are the only two species of birds that can fly. There are four species of birds that fly.\n",
      "\n",
      "So, when you get this idea, you're basically talking about one group of birds. Each has two wings, and they have a wingspan of roughly 30 inches. That's four birds of a feather. Four birds of a feather. And those four birds of a feather are different species of birds. The four species of birds that fly are the only four species of birds that can fly. They are the only four species of birds that can fly. That's what we\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "output = generator(\"Hello, today I am learning about\", max_length=50, num_return_sequences=1)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470cdc96-2481-4ba9-ba2d-3f82ce77f54d",
   "metadata": {},
   "source": [
    "### Difference between model, tokenizer, and pipeline.\n",
    "1. Model (gpt2)\n",
    "   - The neural network that has been trained (here: GPT-2).\n",
    "   - It predicts the next tokens/words given the input.\n",
    "\n",
    "2. Tokenizer\n",
    "   - Converts text → numbers (tokens) before feeding to the model, and numbers → text after generation.\n",
    "   - Handles splitting words into subwords (e.g., “learning” → ['learn', 'ing']).\n",
    "   - You don’t explicitly define it here—pipeline automatically loads the right tokenizer for gpt2.\n",
    "\n",
    "3. Pipeline (pipeline(\"text-generation\", model=\"gpt2\"))\n",
    "   - A high-level wrapper that ties everything together:\n",
    "       - Loads the model and tokenizer.\n",
    "       - Handles preprocessing (tokenization), running the model, and postprocessing (detokenization).\n",
    "   - Makes it easy to run tasks without dealing with the low-level details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

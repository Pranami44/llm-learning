# LLM Reasoning

![Screenshot 2025-09-16 at 1.02.35‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-16_at_1.02.35_PM.png)

![Screenshot 2025-09-16 at 1.11.34‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-16_at_1.11.34_PM.png)

![Screenshot 2025-09-16 at 1.34.04‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-16_at_1.34.04_PM.png)

## Processes to train models

## Simple Prompting

**Back Propagation**

If the next prediction is not the word "future," we need to adjust parameters. In machine learning, that's called 

[Back Propagation](https://www.notion.so/Back-Propagation-2714c0b816088002b84ce86f929c51fd?pvs=21)

![Screenshot 2025-09-17 at 12.58.22‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_12.58.22_PM.png)

**Few Shot Prompting**

For this problem, we can simply concatenate all the examples we have had as the input and also concatenate with the test example, "Barack Obama" here. We can try this with any LLM and see what happened. And probably, you can see, it's **not correct**. Because K is the last letter of "Barack," and A is the last letter of "Obama." The output should be KA. This is called a 

[Few Shot Prompting.](https://www.notion.so/Few-Shot-Prompting-2714c0b8160880e19a5fe247103cd69a?pvs=21)

**Fix:**  We just need to add **reasoning process** before answer. 

![Screenshot 2025-09-17 at 1.10.38‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_1.10.38_PM.png)

**Result:** And you see, we'll get a perfect response from the large language models. We cannot imagine any machine learning method can achieve this perfect.

## Intermediate Steps

### Natural Language Rationale:

Derive the final answer through series of small steps. Trained a sequence-to-sequence model from scratch.

Here are some examples:

![Screenshot 2025-09-17 at 1.24.26‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_1.24.26_PM.png)

![Screenshot 2025-09-17 at 1.25.57‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_1.25.57_PM.png)

![Screenshot 2025-09-17 at 1.28.59‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_1.28.59_PM.png)

### Chain-of-Thought (CoT) Prompting

It means multi-step reasoning.

Chain-of-thought prompting is a way of asking an LLM to **show its reasoning steps** instead of jumping straight to the answer.

**How it works**

- **Normal prompt (direct answer):**
    - Q: *If there are 3 cars, each with 4 wheels, how many wheels total?*
    - A: *12*
- **Chain-of-thought prompt:**
    - Q: *If there are 3 cars, each with 4 wheels, how many wheels total? Let‚Äôs think step by step.*
    - A:
        1. Each car has 4 wheels.
        2. There are 3 cars.
        3. 3 √ó 4 = 12.
        4. Final answer: **12**.

**Why it works**

- Helps the model break problems into smaller parts.
- Reduces mistakes in reasoning tasks (math, logic, planning).
- Makes the process more **transparent**, so you can see if it messed up.

<aside>
üí°

What matters from all of these is **Intermediate Steps**

Regardless of training, fine tuning or prompting, when provided with examples that include intermediate steps, LLMs will generate responses that also include intermediate steps.

</aside>

### Least-to-Most Prompting

- In this work, we enable easy-to-hard generalization by decomposition.
- Decompose complex tasks into simpler tasks. And recombine the decomposed task

![Screenshot 2025-09-17 at 1.55.29‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_1.55.29_PM.png)

Some examples of tasks using this technique are:

![Screenshot 2025-09-17 at 2.00.55‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_2.00.55_PM.png)

![Screenshot 2025-09-17 at 2.37.52‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_2.37.52_PM.png)

### **LLMs as Analogical Reasoners (**Modification of zero-shot approach)

- Recall a related problem and solve this one

![Screenshot 2025-09-17 at 3.05.34‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_3.05.34_PM.png)

The key idea here is, adaptively generate relevant examples and the knowledge for each given problem instead of using a big set of examples as in manual chain-of-thought prompting.

> Is it possible to trigger a step-by-step reasoning, even without using any prompt like ‚Äúlet's think step by step‚Äù?
: Fine Tuning
> 

That means they've already used many examples in the data mixture for training or tuning.

### Chain-of-thought Reasoning without Prompting

Without prompting-- that means without seeing anything. Just give the problem to the model.

**Example of Greedy decoding:**

![Screenshot 2025-09-17 at 3.35.28‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_3.35.28_PM.png)

For this example, we see the approach actually is very simple. 

- Decoding, in the first step, we look at all possible tokens. Here are listed five tokens here. So we started the first top-5 tokens and then continue greedy decoding.
- So the first one is a 5 apples. The first token is 5, and the next token was five apples.
- top-2 tokens is I, then the fourth generation will be I have 3 apples. My dad has 2 more apples than me, and so he has 5 apples.

So we didn't see anything about reasoning here, but the model can do some reasoning if we started from different tokens.

**Another example using CoT Decoding:**

![Screenshot 2025-09-17 at 3.45.31‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_3.45.31_PM.png)

If you look at the probability, on the first row here, Nicolas Cage was born in odd year. And the probability is quite low. However, if you see-- if there's a reasoning path, like the last one, Cage was born in 1964, an even year. There's a reasoning process here. And then probability finally jump to 0.978. For that 20 straight, even or odd the probabilities are very low.

**Key Obsevation:** Pre-trained LLMs have had responses with step-by-step reasoning among generations started with the top-k tokens. We don't need to use any prompts here. And higher confidence in decoding the final answer on a step-by-step reasoning path is present.

<aside>
üí°

On comparison between Greedy decoding and the chain-of-thought decoding, we see that the chain-of-thought decoding performs much better.

</aside>

> So is generating intermediate steps better than  giving direct answers?
: Yes
> 

### Special Decoding

- LLMs are probalistic models of generating next tokens. They are not humans.

![Screenshot 2025-09-17 at 4.09.17‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_4.09.17_PM.png)

P ‚Üí probabilty

We have to make sure final answer is correct and then look at the reasoning path. They're two different objectives.

![Screenshot 2025-09-17 at 4.11.32‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_4.11.32_PM.png)

So given a math problem, you could have found different solutions which lead to the same answer when you do the summation here.

How to compute sum? ‚Üí Sampling

### Self-Consistency (1st principles in machine learning)

Self-consistency is a technique used to improve the **reliability of model outputs**.

Instead of relying on a **single answer** from the model, you let the model **generate multiple reasoning paths** (multiple possible answers), and then you **aggregate or choose the most consistent answer**.

**Why do we need it?**

LLMs can sometimes:

- Make reasoning mistakes.
- Follow one path of logic that leads to the wrong answer.
- Give different answers if asked the same question multiple times.

Self-consistency reduces this randomness by **checking multiple reasoning chains**.

**How it works (Step by Step)**

1. Ask the model a reasoning question, but allow it to sample multiple outputs (e.g., temperature > 0).
2. Collect several answers.
3. Look for the most common or consistent one.

<aside>
üí°

Self-consistency = multiple tries + majority vote = more reliable answers.

</aside>

![Screenshot 2025-09-17 at 4.50.36‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_4.50.36_PM.png)

free-from ‚Üí Questions that can have many answers like reasoning paragraph ones.

![Screenshot 2025-09-17 at 4.53.55‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_4.53.55_PM.png)

## Limitations

### 1. LLMs Can Be Easily Distracted by Irrelevant Context

![Screenshot 2025-09-17 at 5.02.20‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_5.02.20_PM.png)

### 2. LLMs cannot self-correct reasoning yet

![Screenshot 2025-09-17 at 5.06.12‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_5.06.12_PM.png)

‚ö†Ô∏è¬†**Key issue:** While allowing LLMs to review their generated response, can help correct inaccurate answers, it will also risk changing correct answers into incorrect ones.

**Solution: Using Oracle**

![Screenshot 2025-09-17 at 5.10.43‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_5.10.43_PM.png)

Oracle ‚Üí Oracle means you only prompt LLMs to correct the answer when the answer is wrong.

The problem is that the model doesn't know if the answer is correct or wrong. You tell them, the answer is wrong and please correct it. This ability is called multi-LLM debate. You could find using multiple LLMs debating each other until it achieve agreement or consensus.

<aside>
üí°

Oracle feedback is needed for LLM to self-correct.

</aside>

Then we lead to our work as S**elf-debug**

![Screenshot 2025-09-17 at 5.17.47‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_5.17.47_PM.png)

### 3. Premise order matters in LLM reasoning

**Mathematical Example**

![Screenshot 2025-09-17 at 5.22.38‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_5.22.38_PM.png)

Here, we just did a simple trick. We are given this original GSM8K problem. We reorder the sentence a little bit and see if the model still can solve it.

The model just know how to solve the problem **sequentially**. They couldn't go back and forth.

**Logical Reasoning Example**

![Screenshot 2025-09-17 at 5.28.57‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/Screenshot_2025-09-17_at_5.28.57_PM.png)

> It's really important to design experiments when doing research.
> 

## Summary

![Screenshot 2025-09-17 at 5.30.38‚ÄØPM.png](LLM%20Reasoning%202704c0b81608803290d6c49a345b320a/846c76e9-d496-4243-a93d-6c3477b64048.png)
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "077b7378-faef-4025-9fc5-234d767e5789",
   "metadata": {},
   "source": [
    "# LLM Reasoning Summary\n",
    "\n",
    "LLMs generate responses by predicting likely next tokens, enabling reasoning through patterns learned from data. Key reasoning approaches include:\n",
    "\n",
    "- **Zero-shot:** Answering tasks without examples.  \n",
    "- **Few-shot:** Learning from a few examples in the prompt.  \n",
    "- **Chain-of-Thought (CoT):** Breaking problems into intermediate reasoning steps.  \n",
    "- **Analogical Reasoning:** Solving new problems using similarities to known cases.  \n",
    "- **Decoding Strategies:** Techniques like greedy, beam search, or sampling to generate outputs.  \n",
    "- **Self-Consistency:** Sampling multiple reasoning paths and selecting the most consistent answer.  \n",
    "\n",
    "**Limitations:**  \n",
    "- **Irrelevant Context:** May include unrelated or distracting information from long prompts.  \n",
    "- **Premise Order Sensitivity:** Changing the order of facts or statements can lead to different or incorrect conclusions.  \n",
    "- **Self-Correction Challenges:** May fail to identify or correct mistakes without expli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2342932-414d-4b00-8e92-1400a10414d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LLM Reasoning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717d59bd-e249-48e4-8a4c-ff0027a5f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9cebb1-2547-4d8f-93c4-b7c8971c9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize text-generation pipeline\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef092a5-624e-4f05-b961-4ced8ba3a6aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reasoning Experiment 1: Simple Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857266ee-6280-4aa1-9f07-620b9151af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Step by step: If I have 10 apples and eat 3, then buy 5 more, how many apples do I have?\n",
      "Output 1: Step by step: If I have 10 apples and eat 3, then buy 5 more, how many apples do I have?\n",
      "\n",
      "I am not sure if a specific apple is worth buying because it is not really obvious. If a specific apple is worth buying, then I would say to be careful. If you have a limited amount of apples, just buy them.\n",
      "\n",
      "One of the most important things in buying apples\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Step by step: If I have 10 apples and eat 3, then buy 5 more, how many apples do I have?\"\n",
    "output1 = gen(prompt1, \n",
    "              max_new_tokens=60,\n",
    "              truncation=True,\n",
    "              pad_token_id=50256\n",
    "             )[0]['generated_text']\n",
    "\n",
    "print(\"Prompt 1:\", prompt1)\n",
    "print(\"Output 1:\", output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecbd65-a4fa-47df-a923-5cc1a3d6041f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reasoning Experiment 2: Tricky Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd3b5bf-9805-4173-9030-5c985bbe566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 2: Step by step: I have 2 bananas. I eat 2 and then buy 4. How many bananas do I have now?\n",
      "Output 2: Step by step: I have 2 bananas. I eat 2 and then buy 4. How many bananas do I have now? 6 bananas. Now I have 2 bananas. How many bananas do I have now? 1 banana. How many bananas do I have now? 1 banana. How many bananas do I have now? 2 bananas. Now I have 2 bananas. How many bananas do I have now? 2 bananas. Now\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"Step by step: I have 2 bananas. I eat 2 and then buy 4. How many bananas do I have now?\"\n",
    "output2 = gen(prompt2, \n",
    "              max_new_tokens=60,\n",
    "              truncation=True,\n",
    "              pad_token_id=50256\n",
    "             )[0]['generated_text']\n",
    "print(\"\\nPrompt 2:\", prompt2)\n",
    "print(\"Output 2:\", output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9599e2-20d2-491b-b8e4-4905ba457efb",
   "metadata": {},
   "source": [
    "### Reasoning Experiment 3: Text / Story Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b8736b-8296-4e91-88f3-8bca53e2c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 3: Step by step: Explain why the sky is blue in simple terms.\n",
      "Output 3: Step by step: Explain why the sky is blue in simple terms.\n",
      "\n",
      "How long are there days of the year in the sky?\n",
      "\n",
      "The average amount of time we spend in the sky is:\n",
      "\n",
      "Weekday 1: 3 hours\n",
      "\n",
      "Weekday 2: 2 hours\n",
      "\n",
      "Weekday 3: 1 hour\n",
      "\n",
      "Weekday 4: 1 hour\n",
      "\n",
      "Weekday 5: 1 hour\n",
      "\n",
      "If you get this far, you'll almost certainly need to take a short break before you get to work.\n",
      "\n",
      "How long do the days of\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"Step by step: Explain why the sky is blue in simple terms.\"\n",
    "output3 = gen(prompt3, \n",
    "              max_new_tokens=100,\n",
    "              truncation=True,\n",
    "              pad_token_id=50256\n",
    "             )[0]['generated_text']\n",
    "print(\"\\nPrompt 3:\", prompt3)\n",
    "print(\"Output 3:\", output3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ba6f9-62ae-44e2-b605-60dc4fe8d503",
   "metadata": {},
   "source": [
    "### Observations\n",
    "**Which prompts worked?**\n",
    "\n",
    "Prompt 3 (Sky explanation) ‚Äì Partially worked\n",
    "- Output attempts to explain the sky, mentions ‚Äúblue‚Äù, ‚ÄúEarth‚Äù, ‚Äúeyes‚Äù.\n",
    "- But it‚Äôs incorrect and repetitive, not scientifically accurate.\n",
    "\n",
    "**Which failed?**\n",
    "\n",
    "Prompt 1 & Prompt 2 (Math) ‚Äì Failed\n",
    "- Output doesn‚Äôt correctly compute ‚Äú10 ‚àí 3 + 5 = 12‚Äù or ‚Äú2 ‚àí 2 + 4 = 4‚Äù.\n",
    "- GPT-2 repeats the prompt, then produces irrelevant text.\n",
    "  \n",
    "**Insights on why reasoning fails in GPT-2 (or smaller models)**\n",
    "- LLMs like GPT-2 are good at mimicking language, not performing calculations or logic.\n",
    "- Prompt engineering helps a little, but model limitations dominate.\n",
    "- This motivates why we need agents, tool use, and larger LLMs for reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393e2a1-bfe2-4350-b9d2-0743cbda6759",
   "metadata": {},
   "source": [
    "## Reasoning Demo\n",
    "\n",
    "We first tried reasoning with GPT-2 but it failed completely.  \n",
    "Here, we use a **small reasoning-capable model** (`bloomz-560m`) to demonstrate step-by-step reasoning.  \n",
    "\n",
    "> Note: Small models may not always give correct results for math/logic, but they attempt reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1a7d005-b4a4-4d14-bb65-586b475d7e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Step by step: If I have 12 oranges, give 4 to a friend, then buy 6 more, how many do I have?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: Step by step: If I have 12 oranges, give 4 to a friend, then buy 6 more, how many do I have? 12 \n",
      "\n",
      "Prompt: Step by step: The Eiffel Tower is in Paris. Paris is in which country?\n",
      "Model Output: Step by step: The Eiffel Tower is in Paris. Paris is in which country? France \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Small reasoning-capable model for demo (fast, no login required)\n",
    "reasoner = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"bigscience/bloomz-560m\",\n",
    "    device=\"cpu\"  # use \"cuda\" if you have GPU\n",
    ")\n",
    "\n",
    "# Prompts for demonstration\n",
    "prompts = [\n",
    "    \"Step by step: If I have 12 oranges, give 4 to a friend, then buy 6 more, how many do I have?\",\n",
    "    \"Step by step: The Eiffel Tower is in Paris. Paris is in which country?\"\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    print(\"Prompt:\", p)\n",
    "    output = reasoner(p, max_length=128, temperature=0.2)\n",
    "    print(\"Model Output:\", output[0][\"generated_text\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94852c63-98ad-4fbd-b675-44078c3d75b5",
   "metadata": {},
   "source": [
    "### üìù Observations\n",
    "- GPT-2 fails to solve these prompts.  \n",
    "- `bloomz-560m` attempts reasoning and can give approximate answers.  \n",
    "- Larger models like Falcon-7B, Mistral-7B, or LLaMA-2 are needed for **accurate reasoning**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

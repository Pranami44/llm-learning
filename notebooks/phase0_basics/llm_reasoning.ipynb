{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "077b7378-faef-4025-9fc5-234d767e5789",
   "metadata": {},
   "source": [
    "# LLM Reasoning Summary\n",
    "\n",
    "LLMs generate responses by predicting likely next tokens, enabling reasoning through patterns learned from data. Key reasoning approaches include:\n",
    "\n",
    "- **Zero-shot:** Answering tasks without examples.  \n",
    "- **Few-shot:** Learning from a few examples in the prompt.  \n",
    "- **Chain-of-Thought (CoT):** Breaking problems into intermediate reasoning steps.  \n",
    "- **Analogical Reasoning:** Solving new problems using similarities to known cases.  \n",
    "- **Decoding Strategies:** Techniques like greedy, beam search, or sampling to generate outputs.  \n",
    "- **Self-Consistency:** Sampling multiple reasoning paths and selecting the most consistent answer.  \n",
    "\n",
    "**Limitations:**  \n",
    "- **Irrelevant Context:** May include unrelated or distracting information from long prompts.  \n",
    "- **Premise Order Sensitivity:** Changing the order of facts or statements can lead to different or incorrect conclusions.  \n",
    "- **Self-Correction Challenges:** May fail to identify or correct mistakes without expli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2342932-414d-4b00-8e92-1400a10414d3",
   "metadata": {},
   "source": [
    "# LLM Reasoning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717d59bd-e249-48e4-8a4c-ff0027a5f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize text-generation pipeline\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef092a5-624e-4f05-b961-4ced8ba3a6aa",
   "metadata": {},
   "source": [
    "### Reasoning Experiment 1: Simple Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857266ee-6280-4aa1-9f07-620b9151af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Step by step: If I have 10 apples and eat 3, then buy 5 more, how many apples do I have?\n",
      "Output 1: Step by step: If I have 10 apples and eat 3, then buy 5 more, how many apples do I have?\n",
      "\n",
      "You can make this math work by going to the calculator on this site and just multiplying by 10 until you get to 10.\n",
      "\n",
      "How many apples per apple should I buy?\n",
      "\n",
      "You can buy more apples if you want. The price must be in the range of $1.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Step by step: If I have 10 apples and eat 3, then buy 5 more, how many apples do I have?\"\n",
    "output1 = gen(prompt1, \n",
    "              max_new_tokens=60,\n",
    "              truncation=True,\n",
    "              pad_token_id=50256\n",
    "             )[0]['generated_text']\n",
    "\n",
    "print(\"Prompt 1:\", prompt1)\n",
    "print(\"Output 1:\", output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecbd65-a4fa-47df-a923-5cc1a3d6041f",
   "metadata": {},
   "source": [
    "### Reasoning Experiment 2: Tricky Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd3b5bf-9805-4173-9030-5c985bbe566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 2: Step by step: I have 2 bananas. I eat 2 and then buy 4. How many bananas do I have now?\n",
      "Output 2: Step by step: I have 2 bananas. I eat 2 and then buy 4. How many bananas do I have now? I'm not sure because I don't know, but it's probably 2.5. My friend gave me these and it was a really good banana. It's a little strange that I'm not able to eat them, especially when I'm like 5 or 6 days in the fridge. I was\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"Step by step: I have 2 bananas. I eat 2 and then buy 4. How many bananas do I have now?\"\n",
    "output2 = gen(prompt2, \n",
    "              max_new_tokens=60,\n",
    "              truncation=True,\n",
    "              pad_token_id=50256\n",
    "             )[0]['generated_text']\n",
    "print(\"\\nPrompt 2:\", prompt2)\n",
    "print(\"Output 2:\", output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9599e2-20d2-491b-b8e4-4905ba457efb",
   "metadata": {},
   "source": [
    "### Reasoning Experiment 3: Text / Story Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0b8736b-8296-4e91-88f3-8bca53e2c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 3: Step by step: Explain why the sky is blue in simple terms.\n",
      "Output 3: Step by step: Explain why the sky is blue in simple terms.\n",
      "\n",
      "You'll want to be able to see the sky with both your eyes. The sky is blue because the Earth is blue, and the sky is blue because it is deep blue. If you can see the sky with both eyes, then you can see the sky with both eyes.\n",
      "\n",
      "The blue sky is because the Earth is blue. The Earth is blue because that is the Earth.\n",
      "\n",
      "If you really want to see that blue sky with both eyes, see the Earth with both\n"
     ]
    }
   ],
   "source": [
    "prompt3 = \"Step by step: Explain why the sky is blue in simple terms.\"\n",
    "output3 = gen(prompt3, \n",
    "              max_new_tokens=100,\n",
    "              truncation=True,\n",
    "              pad_token_id=50256\n",
    "             )[0]['generated_text']\n",
    "print(\"\\nPrompt 3:\", prompt3)\n",
    "print(\"Output 3:\", output3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ba6f9-62ae-44e2-b605-60dc4fe8d503",
   "metadata": {},
   "source": [
    "## Observations\n",
    "**Which prompts worked?**\n",
    "\n",
    "Prompt 3 (Sky explanation) – Partially worked\n",
    "- Output attempts to explain the sky, mentions “blue”, “Earth”, “eyes”.\n",
    "- But it’s incorrect and repetitive, not scientifically accurate.\n",
    "\n",
    "**Which failed?**\n",
    "\n",
    "Prompt 1 & Prompt 2 (Math) – Failed\n",
    "- Output doesn’t correctly compute “10 − 3 + 5 = 12” or “2 − 2 + 4 = 4”.\n",
    "- GPT-2 repeats the prompt, then produces irrelevant text.\n",
    "  \n",
    "**Insights on why reasoning fails in GPT-2 (or smaller models)**\n",
    "- LLMs like GPT-2 are good at mimicking language, not performing calculations or logic.\n",
    "- Prompt engineering helps a little, but model limitations dominate.\n",
    "- This motivates why we need agents, tool use, and larger LLMs for reasoning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
